{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Jacobo Casado de Gracia \n",
        "Apartado de entrenamiento y evaluación de los modelos del trabajo de clasificación de Diatomeas. Implementado en Google Colab."
      ],
      "metadata": {
        "id": "31DysfL9j7Rb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importación de librerías necesarias para el proyecto."
      ],
      "metadata": {
        "id": "YEDk6BfAkBqP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3q54XX31vHb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import seaborn as sns\n",
        "import shutil\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Dense, Activation,Dropout,Conv2D, MaxPooling2D,BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam, Adamax, SGD\n",
        "from tensorflow.keras.metrics import categorical_crossentropy\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.models import Model\n",
        "from keras.applications.efficientnet import *\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En caso de tener los datos en un comprimido .zip, se deja el comando:"
      ],
      "metadata": {
        "id": "VxcZ7fGJkKVa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOfTMUeb1vHd"
      },
      "outputs": [],
      "source": [
        "#!unzip -q \"/content/drive/MyDrive/data_big.zip\" -d \"/content/drive/MyDrive/data_big/\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Funciones ya comentadas en el otro script llamado train_test_val_creation y que se adjuntan y utilizan aquí. \\\\\n",
        "Se podría utilizar un dataframe de un fichero pero se cargan los datos de nuevo debido a que el proceso de entrenamiento es más rápido así. \\\\\n",
        "Si se desea más información acerca de las funciones, recomiendo consultar el fichero comentado anteriormente donde se explica lo que realiza cada una de ellas."
      ],
      "metadata": {
        "id": "db6z5HVWkZJd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "teNZL2Ey1vHd"
      },
      "outputs": [],
      "source": [
        "def get_dataframe(sdir, dir):\n",
        "\n",
        "    ht  = 0\n",
        "    wt = 0\n",
        "    samples = 0\n",
        "    sample_count = 10\n",
        "    filepaths = []\n",
        "    labels = []\n",
        "    classlist = os.listdir(sdir)\n",
        "\n",
        "\n",
        "    for klass in classlist:\n",
        "\n",
        "        classpath = os.path.join(sdir, klass)\n",
        "        flist = os.listdir(classpath)\n",
        "\n",
        "        for i, f in enumerate(flist):\n",
        "\n",
        "            fpath=os.path.join(classpath,f)\n",
        "\n",
        "            try: \n",
        "                img = plt.imread(fpath)\n",
        "                shape = img.shape\n",
        "                filepaths.append(fpath)\n",
        "                labels.append(klass)  \n",
        "\n",
        "\n",
        "                if i < sample_count:\n",
        "                    img = plt.imread(fpath)               \n",
        "                    ht += img.shape[0]\n",
        "                    wt += img.shape[1]\n",
        "                    samples += 1\n",
        "            except:\n",
        "                print ('El archivo ', fpath, ' es una imagen inválida. ')\n",
        "\n",
        "    filepaths = pd.Series(filepaths, name='filepaths')\n",
        "    labels = pd.Series(labels, name='labels')\n",
        "\n",
        "    df = pd.concat([filepaths, labels], axis = 1)\n",
        "    class_count = len(df['labels'].unique())\n",
        "    print('El df tiene ', class_count, ' clases.')\n",
        "\n",
        "    groups = df.groupby('labels')\n",
        "    print('{0:^30s} {1:^13s}'.format('Clase', 'Cantidad de imágenes'))\n",
        "\n",
        "    \n",
        "\n",
        "    for label in df['labels'].unique():\n",
        "          group = groups.get_group(label)      \n",
        "          print('{0:^30s} {1:^13s}'.format(label, str(len(group))))\n",
        "\n",
        "    wave = wt/samples\n",
        "    have = ht/samples\n",
        "    aspect_ratio = have/wave\n",
        "    print ('Altura media: ' ,have, '  Anchura media: ', wave, '  Aspect ratio medio: ', aspect_ratio)\n",
        "    \n",
        "    return df\n",
        "\n",
        "def trim (df, max_size, min_size):\n",
        "    \n",
        "    column = 'labels'\n",
        "    df = df.copy()\n",
        "    original_class_count = len(list(df[column].unique()))   \n",
        "    sample_list = [] \n",
        "    spare_list = []\n",
        "    groups = df.groupby(column)\n",
        "    \n",
        "    for label in df[column].unique(): \n",
        "        \n",
        "        group = groups.get_group(label)\n",
        "        sample_count = len(group)    \n",
        "        \n",
        "        if sample_count > max_size:\n",
        "            strat = group[column]\n",
        "            samples, spare_data = train_test_split(group, train_size = max_size, shuffle = True, random_state = 123, stratify = strat)            \n",
        "            sample_list.append(samples)\n",
        "            spare_list.append(spare_data)\n",
        "            \n",
        "        elif sample_count >= min_size:\n",
        "            sample_list.append(group)\n",
        "            \n",
        "    df = pd.concat(sample_list, axis = 0).reset_index(drop = True)\n",
        "    spare_df = pd.concat(spare_list, axis = 0).reset_index(drop = True)\n",
        "    \n",
        "    final_class_count = len(list(df[column].unique())) \n",
        "    \n",
        "    if final_class_count != original_class_count:\n",
        "        print ('El dataframe se ha reducido. Número de clases original: ', original_class_count,' Número de clases actual: ', final_class_count)\n",
        "        \n",
        "        \n",
        "    print(\"Dataframe trimeado:\")\n",
        "    groups = df.groupby('labels')\n",
        "    print('{0:^30s} {1:^13s}'.format('Clase', 'Número de imágenes'))\n",
        "    \n",
        "    for label in df['labels'].unique():\n",
        "          group = groups.get_group(label)      \n",
        "          print('{0:^30s} {1:^13s}'.format(label, str(len(group))))\n",
        "    \n",
        "    print(\"Dataframe spare:\")\n",
        "    groups = spare_df.groupby('labels')\n",
        "    print('{0:^30s} {1:^13s}'.format('Clase', 'Número de imágenes'))\n",
        "        \n",
        "    for label in spare_df['labels'].unique():\n",
        "      group = groups.get_group(label)      \n",
        "      print('{0:^30s} {1:^13s}'.format(label, str(len(group))))\n",
        "\n",
        "    return df, spare_df\n",
        "\n",
        "def balance(train_df, max_samples, min_samples, working_dir, image_size, fold_var):\n",
        "    \n",
        "    column = 'labels'\n",
        "    \n",
        "    train_df = train_df.copy()       \n",
        "\n",
        "    aug_dir = os.path.join(working_dir, ('aug_fold_' + str(fold_var)))\n",
        "    \n",
        "    if os.path.isdir(aug_dir):\n",
        "        shutil.rmtree(aug_dir)\n",
        "        \n",
        "    os.mkdir(aug_dir)\n",
        "    \n",
        "    for label in train_df['labels'].unique():    \n",
        "        dir_path = os.path.join(aug_dir, label)    \n",
        "        os.mkdir(dir_path)\n",
        "        \n",
        "     \n",
        "    total = 0\n",
        "    gen = ImageDataGenerator(rotation_range = 180, width_shift_range = .2, height_shift_range = .2, zoom_range = .1, horizontal_flip=True, vertical_flip = True, brightness_range=[0.7,1.3])\n",
        "    \n",
        "    groups=train_df.groupby('labels') # Agrupar por cada clase\n",
        "    \n",
        "    for label in train_df['labels'].unique():  # Por cada clase            \n",
        "        group = groups.get_group(label)  # Nos quedamos con las imágenes de esa clase\n",
        "        sample_count = len(group)   # Cuántas imágenes hay en esa clase\n",
        "        if sample_count < max_samples: # Si hay menos imágenes:\n",
        "            \n",
        "            aug_img_count = 0\n",
        "            delta = max_samples-sample_count  # Aumentamos el resto que falta.\n",
        "            print(\"Resta de imágenes: \", str(delta))\n",
        "            target_dir = os.path.join(aug_dir, label)  # Definimos dónde escribir las imágenes.  \n",
        "            aug_gen = gen.flow_from_dataframe(group,  x_col = 'filepaths', y_col = None, target_size = image_size,\n",
        "                                            class_mode = None, shuffle = False, \n",
        "                                            save_to_dir = target_dir, batch_size = 1, save_prefix = 'aug-', color_mode = 'rgb',\n",
        "                                            save_format = 'jpg')\n",
        "            while aug_img_count < delta:\n",
        "                images = next(aug_gen)            \n",
        "                aug_img_count += len(images)\n",
        "                \n",
        "            total += aug_img_count\n",
        "            \n",
        "    print('Imágenes totales creadas debido al aumento de datos:', total)\n",
        "    \n",
        "    # Creamos el dataset aumentado y lo unimos al de entrenamiento original.\n",
        "    if total > 0:\n",
        "        aug_fpaths = []\n",
        "        aug_labels = []\n",
        "        classlist = os.listdir(aug_dir)\n",
        "        \n",
        "        for klass in classlist:\n",
        "            classpath = os.path.join(aug_dir, klass)     \n",
        "            flist = os.listdir(classpath)    \n",
        "            \n",
        "            for f in flist:        \n",
        "                fpath=os.path.join(classpath,f)         \n",
        "                aug_fpaths.append(fpath)\n",
        "                aug_labels.append(klass)\n",
        "                \n",
        "        Fseries = pd.Series(aug_fpaths, name='filepaths')\n",
        "        Lseries = pd.Series(aug_labels, name='labels')\n",
        "        aug_df = pd.concat([Fseries, Lseries], axis = 1)\n",
        "        train_df = pd.concat([train_df,aug_df], axis = 0).reset_index(drop = True)\n",
        "   \n",
        "    print (list(train_df['labels'].value_counts()) )\n",
        "    return train_df\n",
        "\n",
        "class ASK(keras.callbacks.Callback):\n",
        "    \n",
        "    def __init__ (self, model, epochs,  ask_epoch): # Inicializamos el callback\n",
        "        super(ASK, self).__init__()\n",
        "        self.model = model               \n",
        "        self.ask_epoch = ask_epoch\n",
        "        self.epochs = epochs\n",
        "        self.ask = True # Al ponerse true pedimos al usuario que introduzca un número de épocas inicial\n",
        "        \n",
        "    def on_train_begin(self, logs = None): # Esto se ejecuta al principio del entrenamiento.\n",
        "        \n",
        "        if self.ask_epoch == 0: \n",
        "            print('Como mínimo se debe de entrenar 1 época, por lo que se entrenará 1.', flush=True)\n",
        "            self.ask_epoch = 1\n",
        "            \n",
        "        if self.ask_epoch >= self.epochs: # Todavía no hay que preguntar\n",
        "            self.ask=False # No se le pregunta al usuario\n",
        "            \n",
        "        if self.epochs == 1:\n",
        "            self.ask = False # Sólo se ejecuta 1 época. \n",
        "            \n",
        "        else:\n",
        "            print('El entrenamiento seguirá hasta la época', ask_epoch,'. Posteriormente se le le pedirá') \n",
        "            print(' escribir H para parar o volver a insertar un número de épocas para seguir entrenando.')  \n",
        "            \n",
        "        self.start_time = time.time() # Medimos el tiempo desde que empezó el entrenamiento.\n",
        "        \n",
        "    def on_train_end(self, logs = None):   # Se ejecuta al final del entrenamiento  \n",
        "        tr_duration = time.time() - self.start_time   # Mide el tiempo usado en el entrenamiento    \n",
        "        hours = tr_duration // 3600\n",
        "        minutes = (tr_duration - (hours * 3600)) // 60\n",
        "        seconds = tr_duration - ((hours * 3600) + (minutes * 60))\n",
        "        msg = f'Tiempo total de entrenamiento {str(hours)} horas, {minutes:4.1f} minutos, {seconds:4.2f} segundos)'\n",
        "        print (msg, flush=True) # Imprimimos el tiempo total de entrenamiento.\n",
        "        \n",
        "    def on_epoch_end(self, epoch, logs = None):  # Esta función se ejecuta AL FINAL DE CADA ÉPOCA\n",
        "        \n",
        "        if self.ask: \n",
        "            if epoch + 1 == self.ask_epoch: \n",
        "                \n",
        "                print('\\n Introduce H para parar el entrenamiento o un número de épocas con el que continuar entrenando.')\n",
        "                \n",
        "                ans = input()\n",
        "                \n",
        "                if ans == 'H' or ans =='h' or ans == '0': # Preguntamos condición de parada\n",
        "                    print ('Has introducido ', ans, '; el entrenamiento parará en la época ', epoch+1, flush=True)\n",
        "                    self.model.stop_training = True # Paramos el entrenamiento\n",
        "                    \n",
        "                else: # user wants to continue training\n",
        "                    self.ask_epoch += int(ans)\n",
        "                    if self.ask_epoch > self.epochs:\n",
        "                        print('\\nYou specified maximum epochs of as ', self.epochs, ' cannot train for ', self.ask_epoch, flush =True)\n",
        "                    else:\n",
        "                        print ('has introducido ', ans, ' épocas más. \\n El entrenamiento continuará hasta la época ', self.ask_epoch, flush=True)\n",
        "                        \n",
        "def get_model_name(k):\n",
        "    return 'fold'+str(k)+'.h5'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuración de los parámetros de creación del dataset\n",
        "Tamaño de imagen y número máximo y mínimo de imágenes por clase y ruta del directorio donde guardar los modelos y donde cargar los datos."
      ],
      "metadata": {
        "id": "RWyAva7gkqbh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejecución.\n",
        "max_samples = 400\n",
        "min_samples = 10\n",
        "\n",
        "sdir='/content/drive/MyDrive/data_big/'\n",
        "dir = 'big'\n",
        "\n",
        "save_dir = '/content/drive/MyDrive/saved_models/'\n",
        "fold_var = 0\n",
        "\n",
        "img_size = (224,224) \n",
        "\n",
        "save_dir = '/content/drive/MyDrive/saved_models/' # Dónde guardar los datos aumentados."
      ],
      "metadata": {
        "id": "QWxtcCpqn5X3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ch40TaCyzKAE"
      },
      "outputs": [],
      "source": [
        "df = get_dataframe(sdir, dir) # Cargamos el dataframe\n",
        "trim_df, spare_df = trim(df, max_samples, min_samples) # Recortamos a max_samples y min_samples\n",
        "\n",
        "train_df, test_df = train_test_split(trim_df, train_size = .8, shuffle = True, random_state = 123, stratify = trim_df['labels']) # División en entrenamiento y test.\n",
        "\n",
        "print('Tamaño del conjunto de ENTRENAMIENTO: ', len(train_df), '  \\nTamaño del conjunto de TEST: ', len(test_df))\n",
        "\n",
        "print(\"Dataframe train:\")\n",
        "classes = list(train_df['labels'].unique())\n",
        "class_count = len(classes)\n",
        "groups = train_df.groupby('labels')\n",
        "\n",
        "print('{0:^30s} {1:^13s}'.format('Clase', 'Número de imágenes'))\n",
        "for label in train_df['labels'].unique():\n",
        "      group = groups.get_group(label)      \n",
        "      print('{0:^30s} {1:^13s}'.format(label, str(len(group))))\n",
        "\n",
        "test_df = pd.concat([test_df, spare_df]) # Lo que ha sobrado de recortar el df de entrenamiento lo añadimos al test.\n",
        "print(\"Tamaño del conjunto de TEST ACTUALIZADO: \", len(test_df))\n",
        "test_df.to_pickle(\"/content/drive/MyDrive/folds/test\")\n",
        "\n",
        "train_df_labels = train_df['labels']\n",
        "train_df_samples = train_df['filepaths']\n",
        "print('Tamaño del conjunto de ENTRENAMIENTO: ', len(train_df), '  \\nTamaño del conjunto de TEST: ', len(test_df))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 fold Cross-Validation\n",
        "Si se desea cambiar el número de folds, alterar n_splits."
      ],
      "metadata": {
        "id": "UAKdHP6rlFj4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stratified_kf = StratifiedKFold(n_splits = 3, shuffle = True, random_state = 123)\n",
        "\n",
        "train_df_fold = []\n",
        "valid_df_fold = []\n",
        "\n",
        "for train_index, val_index in stratified_kf.split(train_df_samples, train_df_labels):\n",
        "    \n",
        "    print(train_index.size, val_index.size)\n",
        "    \n",
        "    print(\"Número de fold: \", fold_var)\n",
        "\n",
        "    training_data = train_df.loc[train_df.index[train_index]]\n",
        "        \n",
        "    working_dir = r'/content/drive/MyDrive/' \n",
        "    training_data = balance(training_data, max_samples, min_samples, working_dir, img_size, fold_var)\n",
        "    #training_data.to_pickle(\"/content/drive/MyDrive/folds/train_df_fold\" + str(fold_var) + \".pkl\")\n",
        "\n",
        "    \n",
        "    train_df_fold.append(training_data)\n",
        "    \n",
        "    validation_data = train_df.loc[train_df.index[val_index]]\n",
        "    #validation_data.to_pickle(\"/content/drive/MyDrive/folds/validation_df_fold\" + str(fold_var) + \".pkl\")\n",
        "\n",
        "    valid_df_fold.append(validation_data)\n",
        "\n",
        "    fold_var += 1"
      ],
      "metadata": {
        "id": "aJzgsBmxlFMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entrenamiento de los modelos en los folds\n",
        "Se entrena el modelo en cada fold y se guarda un modelo en el directorio especificado anteriormente por cada fold. Se guarda el mejor modelo obtenido en el entrenamiento, teniendo en cuenta el conjunto de validación (gracias a los callbacks, que facilitan el proceso y lo hacen sencillo) \\\\\n",
        "**Los modelos se guardan para evaluarse posteriormente y elegir el mejor.**"
      ],
      "metadata": {
        "id": "0ux-t8SYlNbk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hiperparámetros del modelo.\n",
        "lr = .001 \n",
        "dropout = 0.0\n",
        "batch_size = 32\n",
        "epochs = 40\n",
        "pooling = \"avg\"\n",
        "ask_epoch = 30 # Para el callback ASK. Nos preguntará en la época 30 si queremos continuar"
      ],
      "metadata": {
        "id": "ft7znCWBlZsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entrenamiento con las redes de la familia EfficientNet"
      ],
      "metadata": {
        "id": "fu3FaIoDlq-O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fq5Mpjl20Pbr"
      },
      "outputs": [],
      "source": [
        "img_shape = (img_size[0], img_size[1], 3)\n",
        "\n",
        "t_and_v_gen = ImageDataGenerator()\n",
        "trgen = ImageDataGenerator()\n",
        "\n",
        "for i in range (0, fold_var - 1):\n",
        "    \n",
        "    print(\"Número de fold procesado: \", i)\n",
        "    \n",
        "    training_data = train_df_fold[i]\n",
        "    print(\"Datos de entrenamiento: \", len(training_data))\n",
        "    classes = list(training_data['labels'].unique())\n",
        "    class_count = len(classes)\n",
        "    validation_data = valid_df_fold[i]\n",
        "    print(\"Datos de validación: \", len(validation_data))\n",
        "    \n",
        "    train_gen = t_and_v_gen.flow_from_dataframe(training_data, x_col = 'filepaths', y_col = 'labels', target_size = img_size,\n",
        "                                       class_mode = 'categorical', color_mode = 'rgb', shuffle = True, batch_size = batch_size)\n",
        "    valid_gen = t_and_v_gen.flow_from_dataframe(validation_data, x_col = 'filepaths', y_col = 'labels', target_size = img_size,\n",
        "                                       class_mode = 'categorical', color_mode = 'rgb', shuffle = False, batch_size = batch_size)\n",
        "\n",
        "\n",
        "    model_name = 'EfficientNetV2B0' # Para guardar el modelo con este nombre. El modelo se cambia en la línea de abajo.\n",
        "    base_model=tf.keras.applications.efficientnet_v2.EfficientNetV2B0(include_top=False, weights=\"imagenet\",input_shape=img_shape, pooling=pooling) # Modelo a usar\n",
        "\n",
        "    base_model.trainable = True\n",
        "\n",
        "    x = base_model.output\n",
        "\n",
        "    output = Dense(class_count, activation='softmax')(x)\n",
        "    model = Model(inputs=base_model.input, outputs=output)\n",
        "    model.compile(Adam(learning_rate=lr), loss='categorical_crossentropy', metrics=['accuracy']) \n",
        "    \n",
        "    model_name = save_dir+model_name+\"_fold_\" + str(i) + \"_lr_\" + str(lr) + \"_bs_\" + str(batch_size) + \"_pooling_\" + pooling + \"_samples_\" + str(max_samples) + \".h5\"\n",
        "\n",
        "    checkpoint = tf.keras.callbacks.ModelCheckpoint(model_name, \n",
        "\t\t\t\t\t\t\tmonitor='val_accuracy', verbose=1, \n",
        "\t\t\t\t\t\t\tsave_best_only=True, mode='max')\n",
        "    rlronp = tf.keras.callbacks.ReduceLROnPlateau(monitor = \"val_loss\", factor = 0.5, patience = 2, verbose = 1)\n",
        "    estop = tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\", patience = 6, verbose = 1, restore_best_weights = True)\n",
        "    ask = ASK(model, epochs,  ask_epoch)\n",
        "\n",
        "    callbacks = [rlronp, estop, checkpoint, ask]\n",
        "    \n",
        "    history = model.fit(x = train_gen,  epochs = epochs, verbose = 1, callbacks = callbacks,  validation_data = valid_gen)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entrenamiento con AlexNet"
      ],
      "metadata": {
        "id": "UZz7CvLilwj2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hiperparámetros de AlexNet\n",
        "lr = .001 \n",
        "dropout = 0.0\n",
        "batch_size = 32\n",
        "epochs = 40\n",
        "pooling = \"avg\"\n",
        "\n",
        "ask_epoch = 40\n",
        "img_shape = (img_size[0], img_size[1], 3)"
      ],
      "metadata": {
        "id": "CS57Ifgwl1nd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t_and_v_gen = ImageDataGenerator()\n",
        "trgen = ImageDataGenerator()\n",
        "\n",
        "for i in range (0, fold_var - 1):\n",
        "    \n",
        "    print(\"Número de fold procesado: \", i)\n",
        "    \n",
        "    training_data = train_df_fold[i]\n",
        "    print(\"Datos de entrenamiento: \", len(training_data))\n",
        "    classes = list(training_data['labels'].unique())\n",
        "    class_count = len(classes)\n",
        "    validation_data = valid_df_fold[i]\n",
        "    print(\"Datos de validación: \", len(validation_data))\n",
        "    \n",
        "    train_gen = t_and_v_gen.flow_from_dataframe(training_data, x_col = 'filepaths', y_col = 'labels', target_size = img_size,\n",
        "                                       class_mode = 'categorical', color_mode = 'rgb', shuffle = True, batch_size = batch_size)\n",
        "    valid_gen = t_and_v_gen.flow_from_dataframe(validation_data, x_col = 'filepaths', y_col = 'labels', target_size = img_size,\n",
        "                                       class_mode = 'categorical', color_mode = 'rgb', shuffle = False, batch_size = batch_size)\n",
        "\n",
        "\n",
        "    model_name = 'AlexNet_borrar'\n",
        "    model = keras.models.Sequential([\n",
        "    keras.layers.Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(img_shape)),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
        "    keras.layers.Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
        "    keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(4096, activation='relu'),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(4096, activation='relu'),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(class_count, activation='softmax')\n",
        "    ])\n",
        "    model.compile(Adam(learning_rate=lr), loss='categorical_crossentropy', metrics=['accuracy']) \n",
        "    \n",
        "    model_name = save_dir+model_name+\"_fold_\" + str(i) + \"_lr_\" + str(lr) + \"_bs_\" + str(batch_size) + \"_pooling_\" + pooling + \"_samples_\" + str(max_samples) + \".h5\"\n",
        "\n",
        "    checkpoint = tf.keras.callbacks.ModelCheckpoint(model_name, \n",
        "\t\t\t\t\t\t\tmonitor='val_accuracy', verbose=1, \n",
        "\t\t\t\t\t\t\tsave_best_only=True, mode='max')\n",
        "    rlronp = tf.keras.callbacks.ReduceLROnPlateau(monitor = \"val_loss\", factor = 0.5, patience = 2, verbose = 1)\n",
        "    estop = tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\", patience = 6, verbose = 1, restore_best_weights = True)\n",
        "    ask = ASK(model, epochs,  ask_epoch)\n",
        "\n",
        "    callbacks = [rlronp, estop, checkpoint, ask]\n",
        "    \n",
        "    history = model.fit(x = train_gen,  epochs = epochs, verbose = 1, callbacks = callbacks,  validation_data = valid_gen)"
      ],
      "metadata": {
        "id": "1ICWPOm6odwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparativa de modelos.\n",
        "Se usan todos los modelos de saved_models (o del path models en sí) y se evalúan en entrenamiento y validación. Se guarda un archivo excel los resultados obtenidos tanto en train como en validación para cada modelo. \\\\\n",
        "Se ha utilizado este código para generar las tablas de la memoria."
      ],
      "metadata": {
        "id": "K_8G9BxZl9tZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8HxLZ2BwcHP"
      },
      "outputs": [],
      "source": [
        "# comparativa de modelos\n",
        "models = os.listdir('/content/drive/MyDrive/saved_models/')    \n",
        "\n",
        "t_and_v_gen = ImageDataGenerator()\n",
        "\n",
        "rotate_valid_datagen =  ImageDataGenerator(rotation_range = 180)\n",
        "\n",
        "COLUMN_NAMES = ['Model', 'Fold', 'Train_accuracy', 'Train_loss', 'Valid_accuracy', 'Valid_loss', 'Valid_rotate_acc', 'Valid_rotate_loss']\n",
        "results = pd.DataFrame(columns=COLUMN_NAMES)\n",
        "\n",
        "folds =  3\n",
        "\n",
        "for fold in range (0, folds):\n",
        "\n",
        "  validation_data = valid_df_fold[fold]\n",
        "  train_data = train_df_fold[fold]\n",
        "\n",
        "  valid_gen = t_and_v_gen.flow_from_dataframe(validation_data, x_col = 'filepaths', y_col = 'labels', target_size=(img_size),\n",
        "                                        class_mode = 'categorical', color_mode = 'rgb', batch_size = 1, shuffle = False)\n",
        "  train_gen = t_and_v_gen.flow_from_dataframe(train_data, x_col = 'filepaths', y_col = 'labels', target_size=(img_size),\n",
        "                                        class_mode = 'categorical', color_mode = 'rgb', shuffle = False)\n",
        "  valid_r_gen = rotate_valid_datagen.flow_from_dataframe(validation_data, x_col = 'filepaths', y_col = 'labels', target_size=(img_size),\n",
        "                                        class_mode = 'categorical', color_mode = 'rgb', batch_size = 1, shuffle = False)\n",
        "\n",
        "  for model in models:\n",
        "    if not model.startswith('.') and (model.find('fold_'+str(fold)) != -1):\n",
        "      print(\"Evaluando modelo\", model, \" en el fold \", fold)\n",
        "      path = os.path.join('/content/drive/MyDrive/saved_models/', model) \n",
        "      model_pred = tf.keras.models.load_model(path)\n",
        "\n",
        "      train_loss, train_acc = model_pred.evaluate(train_gen, verbose=1)\n",
        "      print('Accuracy en train: {:5.3f}%'.format(100 * train_acc))\n",
        "\n",
        "      valid_loss, valid_acc = model_pred.evaluate(valid_gen, verbose=1)\n",
        "      print('Accuracy en validación: {:5.3f}%'.format(100 * valid_acc))\n",
        "\n",
        "      valid_rotate_loss, valid_rotate_acc = model_pred.evaluate(valid_r_gen, verbose=1)\n",
        "      print('Accuracy en validación rotada aleatoriamente: {:5.3f}%'.format(100 * valid_rotate_acc))\n",
        "\n",
        "      row = {'Model':model, 'Fold':fold, 'Train_accuracy':train_acc, 'Train_loss':train_loss, 'Valid_accuracy':valid_acc, 'Valid_loss':valid_loss, 'Valid_rotate_acc':valid_rotate_acc, 'Valid_rotate_loss':valid_rotate_loss}\n",
        "      results = results.append(row, ignore_index=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exportación a un archivo excel de los resultados"
      ],
      "metadata": {
        "id": "KEVaDyUxmQ94"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Facilita la creación de la memoria."
      ],
      "metadata": {
        "id": "pIWTvjFdPO5-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qs_rmTzEBywo"
      },
      "outputs": [],
      "source": [
        "results_sort = results.sort_values('Model') # Ordenamos por nombre para tener los modelos juntos (fold 0, 1, 2) en el archivo Excel.\n",
        "print(results_sort)\n",
        "results_sort.to_excel(\"/content/drive/MyDrive/model_comparison.xlsx\")  "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "model_training_and_evaluation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}